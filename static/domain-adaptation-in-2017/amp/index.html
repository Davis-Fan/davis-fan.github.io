<head>
    <meta charset="utf-8">

    <title>A Little Review of Domain Adaptation in 2017</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../favicon.png">

    <link rel="shortcut icon" href="../../favicon.png" type="image/png">
    <link rel="canonical" href="../index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Arthur Pesah">
    <meta property="og:type" content="article">
    <meta property="og:title" content="A Little Review of Domain Adaptation in 2017">
    <meta property="og:description" content="This post was first published as a quora answer to the question What are the most significant machine learning advances in 2017? 2017 has been an amazing year for domain adaptation: awesome image-to-image and language-to-language translations have been produced, adversarial methods for DA have made huge progress and very innovative">
    <meta property="og:url" content="http://localhost:2368/domain-adaptation-in-2017/">
    <meta property="og:image" content="http://localhost:2368/content/images/2018/01/cycleGAN-1.png">
    <meta property="article:published_time" content="2018-01-03T20:23:57.000Z">
    <meta property="article:modified_time" content="2018-01-03T21:11:04.000Z">
    <meta property="article:publisher" content="https://www.facebook.com/arthur.pesah">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="A Little Review of Domain Adaptation in 2017">
    <meta name="twitter:description" content="This post was first published as a quora answer to the question What are the most significant machine learning advances in 2017? 2017 has been an amazing year for domain adaptation: awesome image-to-image and language-to-language translations have been produced, adversarial methods for DA have made huge progress and very innovative">
    <meta name="twitter:url" content="http://localhost:2368/domain-adaptation-in-2017/">
    <meta name="twitter:image" content="http://localhost:2368/content/images/2018/01/cycleGAN-1.png">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Arthur Pesah">
    <meta name="twitter:site" content="@artix41">
    <meta property="og:image:width" content="602">
    <meta property="og:image:height" content="289">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Arthur Pesah",
        "logo": "http://localhost:2368/content/images/2017/10/lyon_2017_squared-2.jpg"
    },
    "author": {
        "@type": "Person",
        "name": "Arthur Pesah",
        "image": {
            "@type": "ImageObject",
            "url": "//www.gravatar.com/avatar/b1502e3777b74c61cab68f2ec68a5fb8?s=250&d=mm&r=x",
            "width": 250,
            "height": 250
        },
        "url": "http://localhost:2368/author/arthur/",
        "sameAs": []
    },
    "headline": "A Little Review of Domain Adaptation in 2017",
    "url": "http://localhost:2368/domain-adaptation-in-2017/",
    "datePublished": "2018-01-03T20:23:57.000Z",
    "dateModified": "2018-01-03T21:11:04.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:2368/content/images/2018/01/cycleGAN-1.png",
        "width": 602,
        "height": 289
    },
    "description": "This post was first published as a quora answer to the question What are the most significant machine learning advances in 2017? 2017 has been an amazing year for domain adaptation: awesome image-to-image and language-to-language translations have been produced, adversarial methods for DA have made huge progress and very innovative",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 1.15">
    <link rel="alternate" type="application/rss+xml" title="Arthur Pesah" href="../../rss/index.html">

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,600,400">
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-anim" src="https://cdn.ampproject.org/v0/amp-anim-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../">Arthur Pesah</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">A Little Review of Domain Adaptation in 2017</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../author/arthur/">Arthur Pesah</a></p>
                    <time class="post-date" datetime="2018-01-03">2018-01-03</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="http://localhost:2368/content/images/2018/01/cycleGAN-1.png" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <div class="kg-card-markdown"><p><em>This post was first published as a quora answer to the question <a href="https://www.quora.com/What-are-the-most-significant-machine-learning-advances-in-2017/answer/Arthur-Pesah">What are the most significant machine learning advances in 2017?</a></em></p>
<p>2017 has been an amazing year for domain adaptation: awesome image-to-image and language-to-language translations have been produced, adversarial methods for DA have made huge progress and very innovative algorithms have been proposed to tackle the giant problem of adapting two domains.</p>
<p>By domain adaptation, I mean any algorithm trying to transfer two domains, usually called source and target (for instance paintings and real photos), into a common domain. To do so, one can chose either to translate one domain into the other (e.g. translate paintings to photos) or to find a common embedding between the two domains. When only the source domain has labels and the goal is to predict the labels of the target domain, it’s called unsupervised domain adaptation and that’s where the advances were the most incredible. There are many benchmarks to evaluate a DA algorithm, one of the most common being to predict the labels of SVHN (a dataset of digits built with house numbers) by using MNIST (the most common handwritten digits dataset) and its labels. In a year, the results have passed from 90% (with Domain Transfer Network (DTN)<sup class="footnote-ref"><a href="index.html#fn1" id="fnref1">[1]</a></sup>, which was already a great improvement on previous methods that turned around 82%, like DRCN<sup class="footnote-ref"><a href="index.html#fn2" id="fnref2">[2]</a></sup>) to 99.2% (with self-ensembling DA<sup class="footnote-ref"><a href="index.html#fn3" id="fnref3">[3]</a></sup>). Besides this quantitative analysis, the translations performed by some algorithms released this year are qualitatively amazing, particularly in visual DA and NLP.</p>
<p align="center">

</p>
<p><em>Figure 1. Transfer of SVHN to MNIST by SBADA-GAN<sup class="footnote-ref"><a href="index.html#fn4" id="fnref4">[4]</a></sup>, May 2017. For testing a DA algorithm, one can try to predict the labels of SVHN by only using the labels of MNIST and the unsupervised translation between SVHN and MNIST.</em></p>
<p>Let’s try to summarize how awesome this year has been for domain adaptation.</p>
<h1 id="adversarialdomainadaptation">Adversarial Domain Adaptation</h1>
<p>If 2015 saw the birth of adversarial domain adaptation (with DANN<sup class="footnote-ref"><a href="index.html#fn5" id="fnref5">[5]</a></sup>) and 2016 the birth of GAN-based domain adaptation (with CoGAN<sup class="footnote-ref"><a href="index.html#fn6" id="fnref6">[6]</a></sup> and DTN<sup class="footnote-ref"><a href="index.html#fn2" id="fnref2:1">[2:1]</a></sup> ), 2017 has seen huge improvements and amazing results with these methods. The idea behind adversarial DA is to train two neural networks: a discriminator that tries to separate the target domain from the transformed source domain, and a generator that tries to fool the discriminator to make the source domain look like the target one as much as possible. It’s basically a GAN but taking the source distribution as input instead of a uniform distribution (it is usually called a conditional GAN). I’ve realized a little animation to explain the concept more visually (you can find the code <a href="https://github.com/artix41/transfer-learning-algorithms/tree/master/adda">here</a>):</p>
<p><br>
<em>Figure 2. GAN-based adversarial domain adaptation for two Gaussian domains. The discriminator (background) tries to separate the green distribution from the orange distribution, and the generator modifies the green distribution to fool the discriminator. You can find the code <a href="https://github.com/artix41/transfer-learning-algorithms/tree/master/adda">here</a>.</em></p>
<p>So, what were the “significant advances” in 2017?</p>
<h2 id="adda">ADDA</h2>
<p>First, in February, ADDA<sup class="footnote-ref"><a href="index.html#fn7" id="fnref7">[7]</a></sup> released a generalized theoretical framework for adversarial domain adaptation and achieved a 76.0% score with a simple GAN loss on SVHN → MNIST (which they thought to be the best score for an adversarial network on this task, but they had probably not heard of DTN at the time they submitted their article).</p>
<h2 id="cyclegan">CycleGAN</h2>
<p>A month later, the most important contribution of adversarial DA occurred: the invention of the cycle-consistency loss by <strong>CycleGAN</strong><sup class="footnote-ref"><a href="index.html#fn8" id="fnref8">[8]</a></sup>. This paper was a real revolution. Their idea was to train two conditional GANs, one transferring source to target, and the other target to source, and to consider a new loss, called cycle-consistency, which ensures that if you connect the two networks together it will produce an identity mapping (source → target → source). Their examples of transferring horses to zebra or painting to photos have become really famous and I consider it to be one of the coolest thing of this year! Contrary to other methods like pix2pix<sup class="footnote-ref"><a href="index.html#fn9" id="fnref9">[9]</a></sup>, they didn’t train their algorithm on pairs of images (like a photo of cat and the sketch of this same cat, for pix2pix), but only on the two distributions separated, which makes their results even more impressive.</p>
<p></p>
<p><em>Figure 3. Examples of image-to-image translations with CycleGAN</em></p>
<h2 id="discogan">DiscoGAN</h2>
<p>What’s fun is that a bunch of other papers discovered the cycle-consistency loss simultaneously, between March and May, sometimes giving it another name (like reconstruction loss). It’s for instance the case of <strong>DiscoGAN</strong><sup class="footnote-ref"><a href="index.html#fn10" id="fnref10">[10]</a></sup>, whose loss was a bit different (cross-entropy for the GAN loss instead of MSE for instance) but they also achieved incredible results, by managing to transfer both texture properties (like transforming blonde-haired to brown-haired people, women to men or people with glasses to people without glasses) and geometrical properties (chairs to cars and faces to cars).</p>
<p></p>
<p></p>
<p><em>Figure 4. Examples of image-to-image translations with DiscoGAN</em></p>
<h2 id="dualgan">DualGAN</h2>
<p>It’s also the case of <strong>DualGAN</strong><sup class="footnote-ref"><a href="index.html#fn11" id="fnref11">[11]</a></sup>, who used the cycle loss with a WGAN and other recent tricks on how to train GANs. They applied it on day ←→ night or sketch ←→ photos translations, and here are the results:</p>
<p></p>
<p><em>Figure 5. Examples of image-to-image translations with DualGAN</em></p>
<h2 id="sbadagan">SBADA-GAN</h2>
<p>But those 3 papers didn’t consider any dataset with a task (like classification), so didn’t give any quantitative evaluation of their method. <strong>SBADA-GAN</strong><sup class="footnote-ref"><a href="index.html#fn4" id="fnref4:1">[4:1]</a></sup> did it by adding a classifier at the end of their network in order to predict the labels of both the source and the transformed target sample. During the training, pseudo-labels are assigned to the target samples and contribute to the classification loss. The score obtained for SVHN → MNIST is not very good (~76%, same as ADDA), but they achieved new state-of-the-arts on the opposite transformation (MNIST→SVHN) and on MNIST ←→ USPS (another handwritten-digits dataset very close to MNIST).</p>
<h2 id="gentoadapt">GenToAdapt</h2>
<p>Other kind of adversarial architectures have been tried this year with more success on digits benchmarks, like <strong>GenToAdapt</strong><sup class="footnote-ref"><a href="index.html#fn12" id="fnref12">[12]</a></sup> in April who made the first real state-of-the-art of the year in SVHN → MNIST, with a score of 92.4%. Their technique was basically to use a GAN to generate source images from both source and target samples, and to discriminate both real vs fake samples and the different classes of the source samples (like AC-GAN). The learned embedding is then used to train a third network, C, to directly predict the labels of the input samples. The figure below (from the original paper) is certainly clearer than my explanation:</p>
<p></p>
<p><em>Figure 6. The architecture of GenToAdapt</em></p>
<h2 id="unit">UNIT</h2>
<p>It’s also the case of <strong>UNIT</strong><sup class="footnote-ref"><a href="index.html#fn13" id="fnref13">[13]</a></sup>, an adversarial method proposed by Nvidia. Like in many Nvidia papers, they performed a large bunch of amazing experiments (image-to-image translation between different outside conditions on the road, between GTA and reality, between different breeds of dogs, etc.). They have also tested their algorithm on SVHN → MNIST, and obtained 90.53%, which is very close to DTN score, but they manage to transfer much higher-resolution images. Their technique is based on CoGAN<sup class="footnote-ref"><a href="index.html#fn6" id="fnref6:1">[6:1]</a></sup>, which consists in two GANs, one for generating the source domain and one for the target domain, with weight-sharing for some layers. Nvidia’s main contribution was to replace the generator by a VAE. They indeed show that the VAE loss is equivalent to the cycle-consistency constraint described in the previous papers.</p>
<p></p>
<p><em>Figure 7. Examples of image-to-image translations with UNIT</em></p>
<h2 id="stargan">StarGAN</h2>
<p>However, those architectures are only capable of transferring one source domain to one target domain at a time. But if you have multiple domains, there should be a way to train a network to perform transfers in all the domains. In November <strong>StarGAN</strong><sup class="footnote-ref"><a href="index.html#fn14" id="fnref14">[14]</a></sup> adapted CycleGAN to this so-called multi-source domain adaptation problem. Their results in transferring different hair colors or emotions for the same person were pretty amazing as you can see:</p>
<p></p>
<p><em>Figure 8. Example of multi-domain image translations with StarGAN</em></p>
<h2 id="wordtranslationwithoutparalleldata">Word Translation Without Parallel Data</h2>
<p>It might seem from the examples above that the DA community is putting all its efforts into computer vision (CV). But one of the most impressive (and shared) DA paper of the year is in natural language processing (NLP) : <strong>Word Translation Without Parallel Data</strong><sup class="footnote-ref"><a href="index.html#fn15" id="fnref15">[15]</a></sup>. They basically used adversarial DA to find a common embedding between samples from two languages (source and target), and managed to perform very accurate translations without having trained on any example of translation! If you read the paper, you can notice that the expression “domain adaptation” haven’t been used once… Since most DA folks are into computer vision, it seems that the NLP guys who wrote this paper were not aware that their work entered into domain adaptation. So I think NLP would benefit a great deal by testing on their data all the brand new DA methods that the CV community has invented this year.</p>
<p></p>
<p><em>Figure 9. Alignement of the embedding word spaces of the source (english) and the target (italian) domains.</em></p>
<h2 id="pix2pixhd">Pix2Pix HD</h2>
<p>Finally, I have talked only about unpaired domain adaptation (where you don’t use any pair of corresponding source/target samples during the training), but paired DA has also known a little revolution with <strong>pix2pixHD</strong><sup class="footnote-ref"><a href="index.html#fn16" id="fnref16">[16]</a></sup>. It’s basically an improved version of pix2pix (a conditional GAN trained on pairs of images) with many tricks to make it scalable to bigger images. They trained their network to transform semantic maps into realistic photos of street scenes, as you can see on the animation below:</p>
<p></p>
<p><em>Figure 10. Translation of a semantic map (map of labels) to a real street scene with pix2pixHD</em></p>
<h1 id="embeddingmethods">Embedding methods</h1>
<p>Apart from adversarial DA, many other methods have been tried this year, some of them being very successful. That’s the case of two recent methods which try to find a common embedding between the source and target domains, leading at the end to a single neural network capable of classifying both source and target samples.</p>
<h2 id="associativeda">Associative DA</h2>
<p>The first one is <strong>Associative DA</strong> (\( DA_{assoc} \))<sup class="footnote-ref"><a href="index.html#fn17" id="fnref17">[17]</a></sup> who achieved a score of <strong>97.6%</strong> on SVHN→MNIST. In order to find the best embedding, they used the new trend of 2017… cycle-consistency loss! Yes, again, but this time without any GAN or other adversarial network: they just try to learn an embedding (last layer of a neural network) such that the probability of translating a source sample to a target sample (based on the distance between the two points in the embedding space), then converting back this target sample to another source sample will be high if the two source samples belong to the same class.</p>
<h2 id="selfensemblingda">Self-Ensembling DA</h2>
<p>The second one is <strong>Self-Ensembling DA</strong><sup class="footnote-ref"><a href="index.html#fn3" id="fnref3:1">[3:1]</a></sup>, who really destroyed our benchmark SVHN→MNIST with a score of <strong>99.2%</strong> ! We’ll have to find other benchmarks next year! They did this exploit by adapting Mean Teacher − a method coming from semi-supervised learning that has achieved recent SOTA in this field − to domain adaptation. The idea is to have two networks, a student and a teacher, and to make the weights of the teacher a moving average of all the weights that the student got during training. Then, labeled source samples are used to train the student to be a good classifier, and unlabeled target samples to train the student to be like the teacher (with a consistency loss). You can find a more visual explanation <a href="https://thecuriousaicompany.com/mean-teacher/">here</a>.</p>
<h1 id="optimaltransport">Optimal Transport</h1>
<p>Another kind of method has been developed this year: domain adaptation based on optimal transport. Optimal transport is a huge area of applied mathematics, consisting in finding the best transport plan from one distribution to another, by minimizing the total cost of transporting a source mass to a target point. For instance, if you consider two sets of points (with the same number of points each), source and target, and take as the cost function simply the euclidean distance, optimal transport asks you to associate every source point to a target points, so that the total distance is minimized. Here is the solution for two Gaussian domains:</p>
<p></p>
<p><em>Figure 11. Best transport plan between two Gaussian domains. Each source point is transported to a target point, and the total distance is minimized. This graph has been produced with the library <a href="https://github.com/rflamary/POT">POT</a>.</em></p>
<p>This <a href="https://vincentherrmann.github.io/blog/wasserstein/">blog article</a> is an excellent introduction if you want to learn more about OT.</p>
<p>If you start to understand a bit domain adaptation, I think you can now clearly see the link between OT and DA. The relation between those two fields had been theorized in 2016<sup class="footnote-ref"><a href="index.html#fn18" id="fnref18">[18]</a></sup>, but a very interesting algorithm has come out in 2017: <strong>Joint Distribution Optimal Transportation (JDOT)</strong><sup class="footnote-ref"><a href="index.html#fn19" id="fnref19">[19]</a></sup>. Their method is an iterative process: at each iteration, pseudo-labels are given to every target points (at first using a classifier trained on the source samples). Then, the goal is to transport every source point to a target point, minimizing not only the total distance traveled, but also the number of change of label during the transport (between the label of the source point and the pseudo-label of the target point). I made a visual explanation here : <a href="https://github.com/artix41/transfer-learning-algorithms/blob/master/jdot/README.md">A Visual Explanation of JDOT Algorithm</a>, summarized in this GIF (not sure if understandable without pausing at each step):</p>
<p></p>
<p><em>Figure 12. Animation showing the different steps of the JDOT algorithm. You can find all those images separated and associated to some explanations <a href="https://github.com/artix41/transfer-learning-algorithms/blob/master/jdot/README.md">here</a></em></p>
<h1 id="conclusion">Conclusion</h1>
<p>To sum it up, not only has 2017 destroyed some domain adaptation benchmarks, it has also produced the first high-quality translations from one domain to another (as you can see in all those pictures above). But we can still do much better on many more complicated benchmarks and adapt DA to other areas of machine learning (like reinforcement learning and NLP), so 2018 has all its chances to be as awesome as 2017, and I look forward to see what it gives!</p>
<p>If you want to learn more about domain adaptation, I’m maintaining an updated list of great resources (papers, datasets, results, etc.) about DA and transfer learning on <a href="https://github.com/artix41/awesome-transfer-learning">this GitHub repository</a>.</p>
<p><strong>Disclaimer</strong>: the description of those papers only corresponds to my current understanding of them, so take it with a grain of salt and don’t hesitate to tell me if I am incorrect or imprecise in some of my explanations. Concerning the results I give, they are only the ones given in the original papers and a more rigorous methodology should be used in order to make a real comparison.</p>
<h1 id="references">References</h1>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/pdf/1611.02200.pdf">Unsupervised Cross-domain Image Generation</a> (2016) <a href="index.html#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://arxiv.org/pdf/1607.03516.pdf">Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation</a> (2016) <a href="index.html#fnref2" class="footnote-backref">↩︎</a> <a href="index.html#fnref2:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://arxiv.org/pdf/1706.05208.pdf">Self-ensembling for domain adaptation</a> (2017) <a href="index.html#fnref3" class="footnote-backref">↩︎</a> <a href="index.html#fnref3:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="https://arxiv.org/pdf/1705.08824.pdf">From source to target and back: symmetric bi-directional adaptive GAN</a> (2017) <a href="index.html#fnref4" class="footnote-backref">↩︎</a> <a href="index.html#fnref4:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="https://arxiv.org/pdf/1505.07818.pdf">Domain-Adversarial Training of Neural Networks</a> (2015) <a href="index.html#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="https://arxiv.org/pdf/1606.07536.pdf">Coupled Generative Adversarial Networks</a> (2016) <a href="index.html#fnref6" class="footnote-backref">↩︎</a> <a href="index.html#fnref6:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://arxiv.org/pdf/1702.05464.pdf">Adaptative Discriminative Domain Adaptation</a> (2017) <a href="index.html#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p><a href="https://arxiv.org/pdf/1703.10593">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a> (2017) <a href="index.html#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://arxiv.org/pdf/1611.07004.pdf">Image-to-Image Translation with Conditional Adversarial Networks</a> (2016) <a href="index.html#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p><a href="https://arxiv.org/pdf/1703.05192.pdf">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</a> (2017) <a href="index.html#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p><a href="https://arxiv.org/pdf/1704.02510.pdf">DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</a> (2017) <a href="index.html#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p><a href="https://arxiv.org/pdf/1704.01705.pdf">Generate To Adapt: Aligning Domains using Generative Adversarial Networks</a> (2017) <a href="index.html#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="https://arxiv.org/pdf/1703.00848.pdf">Unsupervised Image-to-Image Translation Networks</a> (2017) <a href="index.html#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p><a href="https://arxiv.org/pdf/1711.09020.pdf">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a> (2017) <a href="index.html#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p><a href="../">Word Translation without Parallel Data</a> (2017) <a href="index.html#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p><a href="https://arxiv.org/pdf/1711.11585.pdf">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a> <a href="index.html#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p><a href="https://arxiv.org/pdf/1708.00938.pdf">Associative Domain Adaptation</a> (2017) <a href="index.html#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p><a href="https://arxiv.org/pdf/1610.04420.pdf">Theoretical Analysis of Domain Adaptation with Optimal Transport</a> (2016) <a href="index.html#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p><a href="https://arxiv.org/pdf/1705.08848.pdf">Joint distribution optimal transportation for domain adaptation</a> (2017) <a href="index.html#fnref19" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
</div>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../">Arthur Pesah</a> © 2018</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
