<head>
    <title>Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference</title>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1">


<link rel="apple-touch-icon" sizes="57x57" href="../assets/img/apple-touch-icon-57x57.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="60x60" href="../assets/img/apple-touch-icon-60x60.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="72x72" href="../assets/img/apple-touch-icon-72x72.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="76x76" href="../assets/img/apple-touch-icon-76x76.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="114x114" href="../assets/img/apple-touch-icon-114x114.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="120x120" href="../assets/img/apple-touch-icon-120x120.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="144x144" href="../assets/img/apple-touch-icon-144x144.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="152x152" href="../assets/img/apple-touch-icon-152x152.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="180x180" href="../assets/img/apple-touch-icon-180x180.png?v=wAAv6Wqe6l">
<link rel="icon" type="image/png" href="../assets/img/favicon-32x32.png?v=wAAv6Wqe6l" sizes="32x32">
<link rel="icon" type="image/png" href="../assets/img/favicon-194x194.png?v=wAAv6Wqe6l" sizes="194x194">
<link rel="icon" type="image/png" href="../assets/img/favicon-96x96.png?v=wAAv6Wqe6l" sizes="96x96">
<link rel="icon" type="image/png" href="../assets/img/android-chrome-192x192.png?v=wAAv6Wqe6l" sizes="192x192">
<link rel="icon" type="image/png" href="../assets/img/favicon-16x16.png?v=wAAv6Wqe6l" sizes="16x16">
<link rel="manifest" href="../assets/img/manifest.json?v=wAAv6Wqe6l">
<link rel="shortcut icon" href="../assets/img/favicon.ico?v=wAAv6Wqe6l">
<meta name="msapplication-TileColor" content="#e74c3c">
<meta name="msapplication-TileImage" content="/assets/img/mstile-144x144.png?v=wAAv6Wqe6l">
<meta name="msapplication-config" content="/assets/img/browserconfig.xml?v=wAAv6Wqe6l">
<meta name="theme-color" content="#e74c3c">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,600%7CRoboto+Slab:300,400" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="../assets/css/uno-zen.css?v=c65048221f">
    <link rel="shortcut icon" href="../favicon.png" type="image/png">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="Arthur Pesah">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference">
    <meta property="og:description" content="This post was first published as a Medium Article for Towards Data Science Introduction to likelihood-free inference and distillation of the paper Recurrent Machines for Likelihood-Free Inference, published at the NeurIPS 2018 Workshop on Meta-Learning. Article jointly written by Arthur Pesah and Antoine Wehenkel Motivation There are usually two ways">
    <meta property="og:url" content="http://localhost:2368/improve-scientific-models/">
    <meta property="og:image" content="http://localhost:2368/content/images/2018/12/cms_coverl.jpg">
    <meta property="article:published_time" content="2018-12-23T17:56:37.000Z">
    <meta property="article:modified_time" content="2018-12-23T18:09:44.000Z">
    <meta property="article:publisher" content="https://www.facebook.com/arthur.pesah">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference">
    <meta name="twitter:description" content="This post was first published as a Medium Article for Towards Data Science Introduction to likelihood-free inference and distillation of the paper Recurrent Machines for Likelihood-Free Inference, published at the NeurIPS 2018 Workshop on Meta-Learning. Article jointly written by Arthur Pesah and Antoine Wehenkel Motivation There are usually two ways">
    <meta name="twitter:url" content="http://localhost:2368/improve-scientific-models/">
    <meta name="twitter:image" content="http://localhost:2368/content/images/2018/12/cms_coverl.jpg">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Arthur Pesah">
    <meta name="twitter:site" content="@artix41">
    <meta property="og:image:width" content="1344">
    <meta property="og:image:height" content="742">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Arthur Pesah",
        "logo": "http://localhost:2368/content/images/2020/01/1qbit-1.png"
    },
    "author": {
        "@type": "Person",
        "name": "Arthur Pesah",
        "image": {
            "@type": "ImageObject",
            "url": "//www.gravatar.com/avatar/b1502e3777b74c61cab68f2ec68a5fb8?s=250&d=mm&r=x",
            "width": 250,
            "height": 250
        },
        "url": "http://localhost:2368/author/arthur/",
        "sameAs": []
    },
    "headline": "Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference",
    "url": "http://localhost:2368/improve-scientific-models/",
    "datePublished": "2018-12-23T17:56:37.000Z",
    "dateModified": "2018-12-23T18:09:44.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:2368/content/images/2018/12/cms_coverl.jpg",
        "width": 1344,
        "height": 742
    },
    "description": "This post was first published as a Medium Article for Towards Data Science Introduction to likelihood-free inference and distillation of the paper Recurrent Machines for Likelihood-Free Inference, published at the NeurIPS 2018 Workshop on Meta-Learning. Article jointly written by Arthur Pesah and Antoine Wehenkel Motivation There are usually two ways",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <script type="text/javascript" src="../public/ghost-sdk.js?v=c65048221f"></script>
<script type="text/javascript">
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "c87af3bac556"
});
</script>
    <meta name="generator" content="Ghost 1.15">
    <link rel="alternate" type="application/rss+xml" title="Arthur Pesah" href="../rss/index.html">
    <link rel="shortcut icon" href="../favicon.png">
  </head>
  <body class="post-template">
    <script type="text/javascript">
      window.disqus_shortname = "arthurpesah";
    </script>
    <script type="text/javascript">
      var disqus_config = function () {
          this.page.url = http://localhost:2368/improve-scientific-models/;
          this.page.identifier = ghost-;
      };
    </script>
    <header id="menu-button" class="expanded">
      <a><i class="icon icon-list"></i></a>
    </header>
    <aside class="cover" style="background: url(https://casper.ghost.org/v1.0.0/images/blog-cover.jpg) center/cover no-repeat fixed">
  <div class="cover container">
    <div class="profile">
      <a id="avatar-link" title="link to homepage for Arthur Pesah" href="../index.html#open">
        <img src="../content/images/2020/01/1qbit-1.png" alt="Arthur Pesah avatar" class="profile avatar rounded hvr-buzz-out">
        <h1 id="profile-title">Arthur Pesah</h1>
        <h3 id="profile-resume"></h3>
      </a>

      <hr class="divider long">
      <p>Master's student in theoretical physics at KTH.  Currently working on quantum machine learning at 1QBit</p>
      <hr class="divider short">
      <div class="navigation">
        <div class="profile contact">
          <nav class="navigation left">
  <ul class="links">
      <li class="nav-intro ">
        <a href="../intro/">Intro</a>
      </li>
      <li class="nav-blog ">
        <a href="../">Blog</a>
      </li>
      <li class="nav-publications ">
        <a href="../publications/">Publications</a>
      </li>
      <li class="nav-talks ">
        <a href="../talks/">Talks</a>
      </li>
      <li class="nav-projects ">
        <a href="../projects/">Projects</a>
      </li>
  </ul>
</nav>

          
<nav class="navigation right">
  <ul class="social expanded">

  <!-- Twitter -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://twitter.com/artix41" title="@artix41 on Twitter">
      <i class="icon icon-social-twitter"></i>
      <span class="label">Twitter</span>
    </a>
  </li>

  <!-- Linkedin -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://www.linkedin.com/in/arthur-pesah" title="Arthur Pesah on LinkedIn">
      <i class="icon icon-social-linkedin"></i>
      <span class="label">Linkedin</span>
    </a>
  </li>

  <!-- Github -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://github.com/artix41" title="artix41 on Github">
      <i class="icon icon-social-github"></i>
      <span class="label">Github</span>
    </a>
  </li>

  <!-- RSS -->
  <li class="social item hvr-grow-rotate">
    <a href="../rss/index.rss" title="Subscribe to RSS">
      <i class="icon icon-rss"></i>
      <span class="label">RSS</span>
    </a>
  </li>

  </ul>
</nav>
          <section class="icon icon-search" id="search-container">
  <hr class="divider short">
  <form target="blank" id="search-form" action="https://www.google.com/#q=site:http://localhost:2368">
    <input type="text" name="search" placeholder="git, css, javascript,..." id="search-field">
  </form>
</section>
        </div>
      </div>
    </div>
  </div>
</aside>

    <main data-no-instant>
      <section class="content">
        

  <article class="post">
    <header>
      <div class="post meta">
        <time datetime="2018-12-23T18:12">23 Dec 2018</time>
        <span class="post tags"></span>


        <span class="post reading-time"> ~ <span></span> read.</span>
      </div>
      <a title="Tweet 'Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference'" href="https://twitter.com/intent/tweet?text=Improve%20your%20Scientific%20Models%20with%20Meta-Learning%20and%20Likelihood-Free%20Inference%20%C2%BB&amp;hashtags=&amp;url=http://localhost:2368/improve-scientific-models/">
        <img id="post-image" src="../content/images/2018/12/cms_coverl.jpg" alt="Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference">
        <h1 class="icon-reverse icon-social-twitter-post" id="post-title">Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference</h1>
      </a>
      <style type="text/css">
        img {
            max-width: 100%;
        }
      </style>
    </header>

    <div id="post-content" class="post">
      <div class="kg-card-markdown"><p><em>This post was first published as a <a href="https://towardsdatascience.com/improve-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa">Medium Article</a> for Towards Data Science</em></p>
<p>Introduction to likelihood-free inference and distillation of the paper <a href="https://arxiv.org/abs/1811.12932">Recurrent Machines for Likelihood-Free Inference</a>, published at the <a href="http://metalearning.ml">NeurIPS 2018 Workshop on Meta-Learning</a>.</p>
<p>Article jointly written by Arthur Pesah and Antoine Wehenkel</p>
<h1 id="motivation">Motivation</h1>
<p>There are usually two ways of coming up with a new scientific theory:</p>
<ul>
<li>Starting from first principles, deducing the consequent laws, and coming up with experimental predictions in order to verify the theory</li>
<li>Starting from experiments and inferring the simplest laws that explain your data.</li>
</ul>
<p>The role of statistics and machine learning in science is usually related to the second kind of inference, also called <em>induction</em>.</p>
<p>Imagine for instance that you want to model the evolution of two populations (let’s say foxes and rabbits) in an environment. A simple model is the <a href="https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations">Lotka-Volterra differential equation</a>: you consider the probability that an event such as “a fox eating a rabbit”, “a rabbit being born”, “a fox being born”, etc. happens in a small time interval, deduce a set of differential equations depending on those probabilities, and predict the evolution of the two animals by solving those equations. By comparing your prediction with the evolution of a real population, you can infer the best model parameters (probabilities) in this environment.</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*t9Lv2LZ6EJiutaVzKoQ3lQ.jpeg" alt=""></p>
<p>Modern theories require <strong>simulations</strong> in order to be linked to observations. It can be either a simple differential equation solver as in the case of the Lotka-Volterra model, or a complex Monte-Carlo simulator as they use in particle physics for instance.</p>
<p>By comparing the results of a simulation, i.e. the predictions of a model, with real data, it is then possible to know the correctness of your model and adjust it accordingly. If this process of going back and forth between the model and the experimental data is usually done manually, the question that any machine learning practitioner would ask is: can we do it automatically? Can we build a machine that takes a tweakable simulator and real data as input, and returns the version of the simulator that fits best some real data?</p>
<p>That’s the object of our recent work [^1], where we trained a neural network to come up with the best sequence of simulator tweaks in order to approximate experimental data, capitalizing on the recent advances in the fields of likelihood-free inference and meta-learning.</p>
<h1 id="likelihoodfreeinference">Likelihood-free inference</h1>
<p>Let’s rephrase our problem in a more formal way. We can model a simulator (also called generative model) by a stochastic function that takes some parameters $\theta$ and returns samples $x$ drawn from a certain distribution (the so-called <em>model</em>).</p>
<p>This formalism applies to any scientific theory that includes randomness, as it’s very often the case in modern science (particle collisions are governed by the law of quantum physics which are intrinsically random, biological processes or chemical reactions often occur in a noisy environment, etc.).</p>
<p>Experimental data consist in a set of points living in the same space as the output of the simulator. The goal of inference is then to find the parameters $\theta$ such that the simulator generate points as close as possible to the real data.</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*328OmNFA4xBuj4xgLQdK9w.png" alt=""></p>
<p>Scientific fields using such simulators include:</p>
<ul>
<li>Population genetics. <strong>Model:</strong> <a href="https://en.wikipedia.org/wiki/Coalescent_theory">Coalescent theory</a>. <strong>Observation:</strong> the DNA of a current population. <strong>Parameters:</strong> the DNA of the common ancestor.</li>
<li>High-energy particle physics. <strong>Model</strong>: <a href="https://en.wikipedia.org/wiki/Standard_Model">Standard Model</a> of particle physics. <strong>Observation:</strong> output of the detector during a collision. <strong>Parameters:</strong> coupling constants of the Standard Model (like the mass of the particles or the strength of the different forces).</li>
<li>Computational neuroscience. <strong>Model:</strong> <a href="https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model">Hodgkin-Huxley model</a>. <strong>Observation:</strong> evolution of the voltage of a neuron after activation. <strong>Parameters</strong>: biophysical parameters of the neuron.</li>
</ul>
<p>So how can we predict the parameters of a simulator given some real observations? Let’s consider the simple example of a Gaussian simulator, that takes a vector $\theta=(\mu,\sigma)$ as parameters and returns samples from the Gaussian distribution $\mathcal{N}(\mu,\sigma)$.</p>
<p>The classical way to infer the parameters of such a simulator is called <em>Maximum Likelihood Estimation (MLE)</em>. The likelihood is defined as the density of the real data under a parametric model. It means that if most data points are located in high density regions, the likelihood will be high. Hence, the best parameters of a model are often the ones that maximize the likelihood of real data. If you are unfamiliar with likelihood-based inference, you can read <a href="https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1">this excellent introduction to the subject</a>.</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*16t2IyuYfARkjea8mfUDTQ.png" alt=""></p>
<p>If you have explicitly access to the underlying probability distribution of your simulator, as well as the gradient of this distribution, you can for instance perform a gradient descent in the parameters space, in order to maximize the likelihood and infer the best parameters of your model.</p>
<p>However, many real-life simulators have an <strong>intractable likelihood</strong>, which means that the explicit probability distribution is too hard to compute (either analytically or numerically) . We must therefore find new ways to infer the optimal parameters without using neither the likelihood function nor its gradient.</p>
<p>To sum it up, we have a black-box stochastic simulator that takes parameters and generates samples from an unknown probability distribution, as well as real data that we are able to compare to the generated ones. Our goal is to find the parameters that lead the simulator to generate data as close as possible to the real ones. This setting is called <strong>likelihood-free inference</strong>.</p>
<h1 id="howcanwelikelihoodfreeinfer">How can we likelihood-free infer?</h1>
<p>Let’s try to come up progressively with a method to solve our problem. The first thing we can do is to start from a random parameter, and simulate the corresponding data:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*Dgh8KZYJ_aAYYI73jy8wUQ.png" alt=""><em>Representation of the two spaces of interest. The true parameter (that we wish to infer) is the red point on the left. The real data correspond to the red cloud of points on the right. We start by choosing a random parameter θ (in gray on the left) and simulating the corresponding data points (in gray on the right)</em></p>
<p>By doing so, we can see how far our generated data are from the real data, but we have no way to know where to move in the parameter-space. Instead, let’s simulate several parameters:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*a6mfrolFz-s9CicMOULgvA.png" alt=""></p>
<p>To do so, we consider a distribution in the parameter-space, called <strong>proposal distribution</strong> and noted $q(\theta|\psi)$. If we choose $q$ to be a Gaussian distribution, we will have $\psi=(\mu,\sigma)$. The first step consists in initializing $\psi$ randomly. In the figure above, we considered $\psi=\mu$ for simplicity. Then, we can perform the following step until convergence:</p>
<ul>
<li>Sampling a few parameters from the proposal distribution: 4 parameters around $\psi$ in the figure.</li>
<li>Generating data from them: the 4 cloud of points on right.</li>
<li>Choosing a good direction to move $\psi$.</li>
</ul>
<p>The third step is the hard one. Intuitively, you would like to move $\psi$ towards the orange and green parameters, since the corresponding predictions (orange and green cloud of points) are the closest to the real data. A set of methods, called <em>natural evolution strategies</em> [^3], allow you to link the performance of each $\theta$ (in terms of similarity between its predictions and the real data) to a direction in the parameter space. A recent paper [^4] use for instance the similarity measure given by a Generative Adversarial Network (GAN) to find the best direction. Even though those algorithms perform well in the general case, one can wonder if for a given simulator, it is not possible to find a better algorithm that would exploit the particular properties of this simulator. That’s where meta-learning comes into play!</p>
<h1 id="metalearningforoptimization">Meta-learning for optimization</h1>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*Wip2SwVt4aMqBPtE2Spffw.png" alt=""></p>
<p>The idea behind meta-learning is to learn how to learn, and in our case to <strong>learn the optimization process</strong>. The main idea, introduced in the paper <a href="https://arxiv.org/abs/1606.04474">Learning to learn gradient descent by gradient descent</a> [^2], is to use a recurrent neural network (RNN) to find the best descent direction at each iteration. Below is an example of sequence of points produced by a randomly initialized RNN:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*5Axutz0l_0TjxRupjEt-1Q.png" alt=""></p>
<p>Each descent direction is random and the last point produced is far from the minimum. During training, it should learn to exploit the gradient information at each point in order to move toward the minimum, giving something like:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*r2Ww8UZAmL3cBRds_p35OQ.png" alt=""></p>
<p>So how to train it? Generate many functions whose you know the minimum, and ask the RNN to minimize the distance between the last point of the sequence and the real minimum.</p>
<h1 id="learningtolearnscientificmodels">Learning to learn scientific models</h1>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*qKC9tH01bkQ_giLs_yGJxg.png" alt=""></p>
<p>In the case of likelihood-free inference, the RNN should return a sequence of proposal parameters $\psi$, given the real observations and the generated cloud of points at each step.</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*La-daJwmoy6ksOpAeIrFkQ.png" alt=""></p>
<p>Now, same question as for learning to learn by gradient descent, how do we train the RNN? Here, the trick is to generate many random parameters θ and to pretend for each one that it is the “true parameter”. We can then simulate each θ generated and obtain a set of “real observations”. The RNN can then be trained by passing it those real observations, looking at its final proposal distribution, and comparing it to the true parameter (that we know because we have generated it).</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*7j8kOuY8p0MlwLXnDVzOEg.png" alt=""></p>
<p>Let’s go through an example to clarify all of that. In the figures below, the proposal distribution is represented in color (red = high density). At the beginning, the RNN is initialized randomly and we evaluate it on our first generated true parameter $\theta$ (in red).</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*_PA7u4qOMkAnVEMm73XPiQ.gif" alt=""></p>
<p>We can then backpropagate the loss into the RNN. After repeating this process for 200 different “true parameters”, this is what is should look like:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*j6eVXNAOHY3rQxD2AqjKWQ.gif" alt=""></p>
<p>We can see that it has learnt to exploit the information of the observation space to move towards the good parameter.</p>
<h1 id="results">Results</h1>
<p>We evaluated our model on different toy simulators, some where we the likelihood is known and some where it is not.</p>
<h2 id="nonlikelihoodfreeproblempoissonsimulator">Non-likelihood-free problem: Poisson Simulator</h2>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*204e7qk6Ml3dmQhwbjx7rw.png" alt=""><em>Example of Poisson distributions for various parameters. Source: Wikipedia</em></p>
<p>The first simulator takes a parameter λ and draw samples from a Poisson distribution $P(\lambda)$. The goal of this example was to see if we obtain comparable performance as the maximum likelihood estimator. Here are the results:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*ALQZ7-7AD6RVjJqQpgXfMA.png" alt=""><em>Comparison of ALFI (Automatic Likelihood-Free Inference, the name of our model), to a maximum likelihood estimator (MLE). Those box-plots represent the distribution of the mean-squared errors between the true parameters and the expected value of the final proposal distributions.</em></p>
<p>We can see that the performance are comparable, even though we didn’t give our model access to the likelihood.</p>
<h2 id="likelihoodfreeproblemparticlephysicssimulator">Likelihood-free problem: Particle Physics Simulator</h2>
<p>To evaluate our model in a true likelihood-free setting, we considered a simplified particle physics model that simulate the collision of an electron and a positron turning into a muon and an antimuon.</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*_Zpy-kvc2wiMW-9KzowoLw.png" alt=""><em>Feynman diagram of the simulated process</em></p>
<p>The parameters of the simulator are the energy of the incoming particles and the Fermi constant, and the output is the angle between the two muons.</p>
<p>To evaluate our method, we compared the real observations with the ones generated by the last parameter found. Here are the results:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*gr7tU9hWoGt8FRkPpZKppQ.png" alt=""><em>Results of our method on a simple particle physics model. Comparison of our the real observations (angles of the produced particles) with the ones generated by our predicted parameter.</em></p>
<h1 id="discussion">Discussion</h1>
<p>We saw what likelihood-free inference is and how meta-learning can be used to solve it by learning the best sequence of simulator tweaks to fit a model to the reality.</p>
<p>As most meta-learning models, a limitation is that it is hard to train. We had trouble scaling our method to more complex simulators, since meta-training requires a lot of simulator calls, which might be very slow in real-world settings. However, as the field of meta-learning makes progress, we hope that new methods will emerge to alleviate this problem and make it more scalable.</p>
<h1 id="references">References</h1>
<p>[1] A. Pesah, A. Wehenkel and G. Louppe, <a href="https://arxiv.org/abs/1811.12932">Recurrent Machines for Likelihood-Free Inference</a> (2018), NeurIPS 2018 Workshop on Meta-Learning</p>
<p>[2] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, N. de Freitas, <a href="https://arxiv.org/abs/1606.04474">Learning to learn gradient descent by gradient descent</a> (2016), NIPS 2016</p>
<p>[3] D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peter, J. Schmidhuber, <a href="http://jmlr.org/papers/v15/wierstra14a.html">Natural Evolution Strategies</a> (2014), Journal of Machine Learning Research (JMLR).</p>
<p>[4] G. Louppe, J. Hermans, K. Cranmer, <a href="https://arxiv.org/abs/1707.07113">Adversarial Variational Optimization of Non-Differentiable Simulator</a> (2017), arXiv e-prints 1707.07113</p>
</div>
    </div>

    <div class="post related">
        <a rel="prev" id="prev-btn" class="btn small square" href="../recent-advances-deep-learning/">← Recent Advances for a Better Understanding of Deep Learning</a>

    </div>

    <section>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://arthurpesah.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
            })();
        </script>

        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

        <footer class="post comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + window.disqus_shortname + '.disqus.com/embed.js';
  console.log(window.disqus_shortname);
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</footer>
    </section>

  </article>


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <footer>
  <span class="copyright">
    © 2020. All rights reserved. Built with <a href="https://github.com/Kikobeats/uno-zen" target="_blank">Uno Zen</a> under <a href="https://ghost.org/" target="_blank">Ghost</a>. Original design by <a href="https://daleanthony.com" target="_blank">Dale Anthony</a>.
  </span>
</footer>
      </section>
    </main>

    

    <script src="../assets/js/uno-zen.common.js?v=c65048221f" type="text/javascript" charset="utf-8"></script>
      <script src="../assets/js/uno-zen.post.js?v=c65048221f" type="text/javascript" charset="utf-8"></script>
    <script>
  if (window.ga_id) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', window.ga_id, 'auto');
    ga('require', 'linkid', 'linkid.js');
    ga('send', 'pageview');
  }
</script>
  </body>
